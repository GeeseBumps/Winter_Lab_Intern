# -*- coding: utf-8 -*-
"""Crawling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1akePl3WkxjZa_G8zdBVDHl_fRaF4I-cn
"""

#크롤링에 필요한 라이브러리
import requests # 웹서버에 request 보낼 수 있는 라이브러리
from bs4 import BeautifulSoup as bs # 파싱을 위한 라이브러리
import pandas as pd
#동적 크롤링은 selenium 이용

#기사에 대한 각 정보는 리스트로 저장

url = []
contents = []


for page in range(1,2):
    url_list = []
    url_header = "https://theminjoo.kr/board/search/briefing?st=title&keyword=윤석열&page="
    url_final = url_header + str(page)
    response = requests.get(url_final)
    html = response.content
    soup = bs(html, "html.parser")
    a_tags = soup.find_all("a") 
    for a_tag in a_tags:
      url_list.append(a_tag['href'])
    for link in url_list:
      if "/board/view/briefing/" in link:
        url.append(link)
links = []
for u in url:
  link = 'https://theminjoo.kr' + u
  response = requests.get(link)
  html = response.text
  soup = bs(html,'html.parser')
  content = soup.select('#content > div.bv_middle > div')[0].get_text()
  content = content.replace('\xa0', " ")
  content = content.replace('\n', " ")
  links.append(link)
  contents.append(content)

print(result)
news_list = {"URL":links, "Content" : contents}
result = pd.DataFrame(news_list)
result.to_excel("theminjoo_yoon.xlsx",index = False)

from google.colab import drive
drive.mount('/content/drive')

import requests # 웹서버에 request 보낼 수 있는 라이브러리
from bs4 import BeautifulSoup as bs # 파싱을 위한 라이브러리
import pandas as pd
url_list1 = []
url_list2 = []
url = "https://theminjoo.kr/board/lists/briefing?page=2"
res = requests.get(url)
html = res.content
soup = bs(html, "html.parser")
a_tags = soup.find_all("a") 
for a_tag in a_tags:
  url_list1.append(a_tag['href'])
for link in url_list1:
  if "/board/view/briefing/" in link:
    url_list2.append(link)
print(url_list2)

